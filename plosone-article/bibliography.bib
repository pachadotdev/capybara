
@misc{stammann,
	title = {Fast and {Feasible} {Estimation} of {Generalized} {Linear} {Models} with {High}-{Dimensional} k-way {Fixed} {Effects}},
	url = {http://arxiv.org/abs/1707.01815},
	doi = {10.48550/arXiv.1707.01815},
	abstract = {We present a fast and memory efficient algorithm for the estimation of generalized linear models with an additive separable k-way error component. The brute force approach uses dummy variables to account for the unobserved heterogeneity, but quickly faces computational limits. Thus, we show how a weighted version of the Frisch-Waugh-Lovell theorem combined with the method of alternating projections can be incorporated into a Newton-Raphson algorithm to dramatically reduce the computational costs. The algorithm is especially useful in situations, where generalized linear models with k-way fixed effects based on dummy variables are computationally demanding or even infeasible due to time or memory limitations. In a simulation study and an empirical application we demonstrate the performance of our algorithm.},
	urldate = {2024-02-16},
	publisher = {arXiv},
	author = {Stammann, Amrei},
	month = jul,
	year = {2018},
	note = {arXiv:1707.01815 [stat]},
	keywords = {Statistics - Applications, Statistics - Computation},
	file = {arXiv Fulltext PDF:/home/pacha/Zotero/storage/D9T9NVA8/Stammann - 2018 - Fast and Feasible Estimation of Generalized Linear.pdf:application/pdf;arXiv.org Snapshot:/home/pacha/Zotero/storage/DC2HDXU5/1707.html:text/html},
}

@misc{czarnowske,
	title = {Inference in {Unbalanced} {Panel} {Data} {Models} with {Interactive} {Fixed} {Effects}},
	url = {http://arxiv.org/abs/2004.03414},
	doi = {10.48550/arXiv.2004.03414},
	abstract = {In this article, we study the limiting behavior of Bai (2009)'s interactive fixed effects estimator in the presence of randomly missing data. In extensive simulation experiments, we show that the inferential theory derived by Bai (2009) and Moon and Weidner (2017) approximates the behavior of the estimator fairly well. However, we find that the fraction and pattern of randomly missing data affect the performance of the estimator. Additionally, we use the interactive fixed effects estimator to reassess the baseline analysis of Acemoglu et al. (2019). Allowing for a more general form of unobserved heterogeneity as the authors, we confirm significant effects of democratization on growth.},
	urldate = {2025-02-27},
	publisher = {arXiv},
	author = {Czarnowske, Daniel and Stammann, Amrei},
	month = apr,
	year = {2020},
	note = {arXiv:2004.03414 [econ]},
	keywords = {Economics - Econometrics},
	file = {Preprint PDF:/home/pacha/Zotero/storage/WCDLGMD9/Czarnowske and Stammann - 2020 - Inference in Unbalanced Panel Data Models with Interactive Fixed Effects.pdf:application/pdf;Snapshot:/home/pacha/Zotero/storage/S6VCAWHE/2004.html:text/html},
}

@article{fernandez16,
	title = {Individual and time effects in nonlinear panel models with large N, T},
	volume = {192},
	issn = {0304-4076},
	url = {https://www.sciencedirect.com/science/article/pii/S0304407615002997},
	doi = {10.1016/j.jeconom.2015.12.014},
	abstract = {We derive fixed effects estimators of parameters and average partial effects in (possibly dynamic) nonlinear panel data models with individual and time effects. They cover logit, probit, ordered probit, Poisson and Tobit models that are important for many empirical applications in micro and macroeconomics. Our estimators use analytical and jackknife bias corrections to deal with the incidental parameter problem, and are asymptotically unbiased under asymptotic sequences where N/T converges to a constant. We develop inference methods and show that they perform well in numerical examples.},
	number = {1},
	urldate = {2025-02-27},
	journal = {Journal of Econometrics},
	author = {Fernández-Val, Iván and Weidner, Martin},
	month = may,
	year = {2016},
	keywords = {Asymptotic bias correction, Dynamic model, Fixed effects, Nonlinear model, Panel data, Time effects},
	pages = {291--312},
	file = {ScienceDirect Full Text PDF:/home/pacha/Zotero/storage/MVB9HP9L/Fernández-Val and Weidner - 2016 - Individual and time effects in nonlinear panel models with large NN,.pdf:application/pdf},
}

@article{fernandez18,
	title = {Fixed {Effects} {Estimation} of {Large}-{T} {Panel} {Data} {Models}},
	volume = {10},
	issn = {1941-1383},
	doi = {10.1146/annurev-economics-080217-053542},
	abstract = {This article reviews recent advances in fixed effects estimation of panel data models for long panels, where the number of time periods is relatively large. We focus on semiparametric models with unobserved individual and time effects, where the distribution of the outcome variable, conditional on covariates and unobserved effects, is specified parametrically while the distribution of the unobserved effects is left unrestricted. In contrast to existing reviews on long panels, we discuss models with both individual and time effects, split-panel jackknife bias corrections, unbalanced panels, distribution and quantile effects, and other extensions. Understanding and correcting the incidental parameter bias caused by the estimation of many fixed effects are our main focuses, and the unifying theme is that the order of this bias is given by the simple formula p/n for all models discussed, with p being the number of estimated parameters and n the total sample size.},
	language = {eng},
	number = {1},
	journal = {Annual review of economics},
	author = {Fernández-Val, Iván and Weidner, Martin},
	year = {2018},
	note = {Publisher: Annual Reviews},
	pages = {109--138},
	file = {Full Text:/home/pacha/Zotero/storage/2MBZX63B/Fernández-Val and Weidner - 2018 - Fixed Effects Estimation of Large-T Panel Data Models.pdf:application/pdf},
}

@article{hahn,
	title = {Bias {Reduction} for {Dynamic} {Nonlinear} {Panel} {Models} with {Fixed} {Effects}},
	volume = {27},
	issn = {0266-4666},
	doi = {10.1017/S0266466611000028},
	abstract = {The fixed effects estimator of panel models can be severely biased because of well-known incidental parameter problems. It is shown that this bias can be reduced in nonlinear dynamic panel models. We consider asymptotics where n and T grow at the same rate as an approximation that facilitates comparison of bias properties. Under these asymptotics, the bias-corrected estimators we propose are centered at the truth, whereas fixed effects estimators are not. We discuss several examples and provide Monte Carlo evidence for the small sample performance of our procedure.},
	language = {eng},
	number = {6},
	journal = {Econometric theory},
	author = {Hahn, Jinyong and Kuersteiner, Guido},
	year = {2011},
	note = {Place: New York, USA
Publisher: Cambridge University Press},
	keywords = {Approximation, Bias, Comparative analysis, Data models, Dynamic modeling, Econometric models, Econometrics, Economic methodology, Economic models, Estimation, Estimation bias, Estimation methods, Estimators, Infinity, Maximum likelihood estimation, Normal distribution, Studies, Time series},
	pages = {1152--1191},
}

@misc{hinz,
	title = {State {Dependence} and {Unobserved} {Heterogeneity} in the {Extensive} {Margin} of {Trade}},
	url = {http://arxiv.org/abs/2004.12655},
	doi = {10.48550/arXiv.2004.12655},
	abstract = {We study the role and drivers of persistence in the extensive margin of bilateral trade. Motivated by a stylized heterogeneous firms model of international trade with market entry costs, we consider dynamic three-way fixed effects binary choice models and study the corresponding incidental parameter problem. The standard maximum likelihood estimator is consistent under asymptotics where all panel dimensions grow at a constant rate, but it has an asymptotic bias in its limiting distribution, invalidating inference even in situations where the bias appears to be small. Thus, we propose two different bias-corrected estimators. Monte Carlo simulations confirm their desirable statistical properties. We apply these estimators in a reassessment of the most commonly studied determinants of the extensive margin of trade. Both true state dependence and unobserved heterogeneity contribute considerably to trade persistence and taking this persistence into account matters significantly in identifying the effects of trade policies on the extensive margin.},
	urldate = {2025-02-27},
	publisher = {arXiv},
	author = {Hinz, Julian and Stammann, Amrei and Wanner, Joschka},
	month = jul,
	year = {2021},
	note = {arXiv:2004.12655 [econ]},
	keywords = {Economics - Econometrics},
	file = {Preprint PDF:/home/pacha/Zotero/storage/ZJM82QT4/Hinz et al. - 2021 - State Dependence and Unobserved Heterogeneity in the Extensive Margin of Trade.pdf:application/pdf;Snapshot:/home/pacha/Zotero/storage/SK3WML2C/2004.html:text/html},
}

@article{neyman,
	title = {Consistent {Estimates} {Based} on {Partially} {Consistent} {Observations}},
	volume = {16},
	issn = {0012-9682},
	doi = {10.2307/1914288},
	language = {eng},
	number = {1},
	journal = {Econometrica},
	author = {Neyman, J. and Scott, Elizabeth L.},
	year = {1948},
	note = {Place: Menasha, Wis
Publisher: JSTOR},
	keywords = {Arithmetic mean, Coefficients, Consistent estimators, Estimation methods, Mathematical independent variables, Radial velocity, Random variables, Stars, Unbiased estimators, Zero},
	pages = {1--32},
}

@book{yotov,
	title = {An {Advanced} {Guide} to {Trade} {Policy} {Analysis}: {The} {Structural} {Gravity} {Model}},
	isbn = {978-92-1-058519-4},
	shorttitle = {An {Advanced} {Guide} to {Trade} {Policy} {Analysis}},
	url = {https://www.un-ilibrary.org/content/books/9789210585194},
	abstract = {This Advanced Guide to Trade Policy Analysis is a complementary follow-up to the original Practical Guide to Trade Policy Analysis. It provides the most recent tools for analysis of trade policy using structural gravity models. Written by experts who have contributed to the development of theoretical and empirical methods in the academic gravity literature and who have rich practical experience in the field, this publication explains how to conduct partial equilibrium estimations as well as general equilibrium analysis with structural gravity models and contains practical guidance on how to apply these tools to concrete policy questions. This Advanced Guide has been developed to contribute to the enhancement of developing countries’ capacity to analyse and implement trade policy. It is aimed at government experts engaged in trade negotiations, as well as graduate students and researchers involved in trade-related study or research.},
	language = {en},
	urldate = {2024-03-20},
	publisher = {United Nations},
	author = {Yotov, Yoto V. and Piermartini, Roberta and Monteiro, José-Antonio and Larch, Mario},
	month = apr,
	year = {2017},
	doi = {10.18356/57a768e5-en},
	file = {Snapshot:/home/pacha/Zotero/storage/IRZ28AV5/9789210585194.html:text/html},
}

@book{hansen,
	address = {Princeton, New Jersey},
	title = {Econometrics},
	isbn = {978-0-691-23589-9},
	abstract = {"An introductory PhD-level textbook for one of the first and most foundational courses every economics graduate student must take"-- Provided by publisher., "Econometrics is the quantitative language of economic theory, analysis, and empirical work, and it has become a cornerstone of graduate economics programs. 'Econometrics' provides graduate and PhD students with an essential introduction to this foundational subject in economics and serves as an invaluable reference for researchers and practitioners. This comprehensive textbook teaches fundamental concepts, emphasizes modern, real-world applications, and gives students an intuitive understanding of econometrics. Covers the full breadth of econometric theory and methods with mathematical rigor while emphasizing intuitive explanations that are accessible to students of all backgrounds ; draws on integrated, research-level datasets, provided on an accompanying website ; discusses linear econometrics, time series, panel data, nonparametric methods, nonlinear econometric models, and modern machine learning ; features hundreds of exercises that enable students to learn by doing ; includes in-depth appendices on matrix algebra and useful inequalities and a wealth of real-world examples ; can serve as a core textbook for a first-year PhD course in econometrics and as a follow-up to Bruce E. Hansen's 'Probability and Statistics for Economists'.--taken from back cover., "This textbook is the second in a two-part series covering the core material typically taught in a one-year Ph.D. course in econometrics. The sequence is : 1. 'Probability and Statistics for Economists' (first volume) ; 2. 'Econometrics' (this volume). 'Econometrics' assumes that students have a background in multivariate calculus, probability theory, linear algebra, and mathematical statistics. A prior course in undergraduate econometrics would be helpful but is not required. The relevant background in probability theory and mathematical statistics is provided in 'Probability and Statistics for Economists'. For reference, the basic tools of matrix algebra and probability inequalites are reviewed in Appendixes A and B. This textbook contains more material than can be covered in a one-semester course. This is intended to provide instructors flexibility concerning which topics to cover, which to cover in depth, and which to cover briefly. Some material is suitable for second-year Ph.D. instruction."--adapted from Preface, page xxv.},
	language = {eng},
	publisher = {Princeton University Press},
	author = {Hansen, Bruce E.},
	year = {2022},
	keywords = {Econometrics},
}

@article{gaure,
	title = {{OLS} with multiple high dimensional category variables},
	volume = {66},
	issn = {0167-9473},
	doi = {10.1016/j.csda.2013.03.024},
	abstract = {A new algorithm is proposed for OLS estimation of linear models with multiple high-dimensional category variables. It is a generalization of the within transformation to arbitrary number of category variables. The approach, unlike other fast methods for solving such problems, provides a covariance matrix for the remaining coefficients. The article also sets out a method for solving the resulting sparse system, and the new scheme is shown, by some examples, to be comparable in computational efficiency to other fast methods. The method is also useful for transforming away groups of pure control dummies. A parallelized implementation of the proposed method has been made available as an R-package lfe on CRAN.
•We generalize the linear within-estimator to two or more category variables.•We describe a general procedure for projecting out dummy-encoded category variables.•Parameters for dummy variables can optionally be estimated.},
	language = {eng},
	journal = {Computational statistics \& data analysis},
	author = {Gaure, Simen},
	year = {2013},
	note = {Publisher: Elsevier B.V},
	keywords = {Alternating projections, Categories, Fixed effect estimator, High dimensional category variables, Kaczmarz method, Multiple fixed effects, Panel data, Two-way fixed effects},
	pages = {8--18},
}

@article{halperin,
	title = {The product of projection operators},
	volume = {23},
	number = {1-2},
	urldate = {2025-02-27},
	journal = {Acta Sci. Math.},
	author = {Halperin, Israel},
	year = {1962},
	pages = {96--99},
	file = {ACTA - ACTA issues:/home/pacha/Zotero/storage/KCALMCHN/showCustomerArticle.html:text/html},
}

@book{neumann,
	address = {Princeton, NJ},
	series = {Annals of {Mathematics} {Studies}},
	title = {Functional {Operators} ({AM}-22), {Volume} 2: {The} {Geometry} of {Orthogonal} {Spaces}. ({AM}-22)},
	isbn = {978-1-4008-8225-0},
	shorttitle = {Functional {Operators} ({AM}-22), {Volume} 2},
	abstract = {Measures and integrals},
	language = {eng},
	number = {287},
	publisher = {Princeton University Press,},
	author = {von Neumann, John},
	year = {1950},
	doi = {10.1515/9781400882250},
	keywords = {Functional analysis, Geometry},
}

@book{mccullagh,
	address = {London ;},
	series = {Monographs on statistics and applied probability ({Series})},
	title = {Generalized linear models},
	isbn = {978-0-412-23850-5},
	language = {eng},
	publisher = {Chapman and Hall},
	author = {McCullagh, P. and Nelder, John A.},
	year = {1983},
	keywords = {Linear models (Statistics)},
}

@book{casella,
	address = {Pacific Grove, CA},
	edition = {2nd ed.},
	title = {Statistical inference},
	isbn = {978-0-534-24312-8},
	language = {eng},
	publisher = {Duxbury/Thomson Learning},
	author = {Casella, George and Berger, Roger},
	year = {2002},
	keywords = {Mathematical statistics, Probabilities},
}

@Manual{rstats,
  title = {`R`: A Language and Environment for Statistical Computing},
  author = {{{R} Core Team}},
  organization = {{R} Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2024},
  url = {https://www.R-project.org/},
}

@Manual{cpp11,
  title = {`cpp11`: A {C++}11 Interface for R's C Interface},
  author = {Davis Vaughan and Jim Hester and Romain François},
  year = {2023},
  note = {R package version 0.4.7},
  url = {https://CRAN.R-project.org/package=cpp11},
}

@article{santos,
  author = "J. M. C. Santos Silva and Silvana Tenreyro",
  title = "The Log of Gravity",
  journal = "The Review of Economics and Statistics",
  volume = "88",
  number = "4",
  pages = "641-658",
  year = "2006",
  doi = "10.1162/rest.88.4.641",
  abstract = "Abstract Although economists have long been aware of Jensen's inequality, many econometric applications have neglected an important implication of it: under heteroskedasticity, the parameters of log-linearized models estimated by OLS lead to biased estimates of the true elasticities. We explain why this problem arises and propose an appropriate estimator. Our criticism of conventional practices and the proposed solution extend to a broad range of applications where log-linearized equations are estimated. We develop the argument using one particular illustration, the gravity equation for trade. We find significant differences between estimates obtained with the proposed estimator and those obtained with the traditional method."
}

@Manual{capybara,
	title = {`capybara`: Fast and Memory Efficient Fitting of Linear Models With High-Dimensional Fixed Effects},
  author = {Mauricio {Vargas Sepulveda}},
	year = {2024},
  note = {R package version 0.6.0, https://github.com/pachadotdev/capybara},
  url = {https://pacha.dev/capybara/},
}

@article{cameron,
  title={Robust inference with multiway clustering},
  author={Cameron, A Colin and Gelbach, Jonah B and Miller, Douglas L},
  journal={Journal of Business \& Economic Statistics},
  volume={29},
  number={2},
  pages={238--249},
  year={2011},
  publisher={Taylor \& Francis}
}

@article{sepulveda,
	title = {cpp11armadillo: {An} {R} package to use the {Armadillo} {C}++ library},
	volume = {30},
	issn = {2352-7110},
	shorttitle = {cpp11armadillo},
	url = {https://www.sciencedirect.com/science/article/pii/S2352711025000548},
	doi = {10.1016/j.softx.2025.102087},
	abstract = {This article introduces ‘cpp11armadillo’, an R package that integrates the highly efficient Armadillo C++ linear algebra library with R through the ‘cpp11’ interface. Designed to offer significant performance improvements for computationally intensive tasks, ‘cpp11armadillo’ simplifies the process of integrating C++ code into R. This package is particularly suited for R users requiring efficient matrix operations, especially in cases where vectorization is not possible. Our benchmarks demonstrate substantial speed gains over native R functions and Rcpp-based setups.},
	urldate = {2025-02-27},
	journal = {SoftwareX},
	author = {Vargas Sepulveda, Mauricio and Schneider Malamud, Jonathan},
	month = may,
	year = {2025},
	keywords = {Armadillo, Benchmarking, C++, Linear algebra, R},
	pages = {102087},
	file = {ScienceDirect Full Text PDF:/home/pacha/Zotero/storage/7BRHLLHG/Sepulveda and Malamud - 2025 - cpp11armadillo An R package to use the Armadillo C++ library.pdf:application/pdf;ScienceDirect Snapshot:/home/pacha/Zotero/storage/4BTJHQE7/S2352711025000548.html:text/html},
}

@article{sanderson,
	title = {Armadillo: a template-based {C}++ library for linear algebra},
	volume = {1},
	issn = {2475-9066},
	shorttitle = {Armadillo},
	url = {https://joss.theoj.org/papers/10.21105/joss.00026},
	doi = {10.21105/joss.00026},
	abstract = {Sanderson et al, (2016), Armadillo: a template-based C++ library for linear algebra, Journal of Open Source Software, 1(2), 26, doi:10.21105/joss.00026},
	language = {en},
	number = {2},
	urldate = {2025-03-04},
	journal = {Journal of Open Source Software},
	author = {Sanderson, Conrad and Curtin, Ryan},
	month = jun,
	year = {2016},
	pages = {26}
}

@article{beyer,
	title = {Reliable benchmarking: requirements and solutions},
	volume = {21},
	issn = {1433-2787},
	shorttitle = {Reliable benchmarking},
	url = {https://doi.org/10.1007/s10009-017-0469-y},
	doi = {10.1007/s10009-017-0469-y},
	abstract = {Benchmarking is a widely used method in experimental computer science, in particular, for the comparative evaluation of tools and algorithms. As a consequence, a number of questions need to be answered in order to ensure proper benchmarking, resource measurement, and presentation of results, all of which is essential for researchers, tool developers, and users, as well as for tool competitions. We identify a set of requirements that are indispensable for reliable benchmarking and resource measurement of time and memory usage of automatic solvers, verifiers, and similar tools, and discuss limitations of existing methods and benchmarking tools. Fulfilling these requirements in a benchmarking framework can (on Linux systems) currently only be done by using the cgroup and namespace features of the kernel. We developed BenchExec, a ready-to-use, tool-independent, and open-source implementation of a benchmarking framework that fulfills all presented requirements, making reliable benchmarking and resource measurement easy. Our framework is able to work with a wide range of different tools, has proven its reliability and usefulness in the International Competition on Software Verification, and is used by several research groups worldwide to ensure reliable benchmarking. Finally, we present guidelines on how to present measurement results in a scientifically valid and comprehensible way.},
	language = {en},
	number = {1},
	urldate = {2024-11-25},
	journal = {International Journal on Software Tools for Technology Transfer},
	author = {Beyer, Dirk and Löwe, Stefan and Wendler, Philipp},
	month = feb,
	year = {2019},
	keywords = {Benchmarking, Competition, Container, Process control, Process isolation, Resource measurement},
	pages = {1--29}
}

@misc{drury,
	title = {A {Deep} {Dive} {Into} {How} {R} {Fits} a {Linear} {Model}},
	url = {http://madrury.github.io/jekyll/update/statistics/2016/07/20/lm-in-R.html},
	urldate = {2024-10-19},
	author = {Drury, Matthew},
	month = jun,
	year = {2016},
}

@article{berge,
	title = {Efficient estimation of maximum likelihood models with multiple fixed-effects: the {R} package {FENmlm}},
	shorttitle = {Efficient estimation of maximum likelihood models with multiple fixed-effects},
	url = {https://ideas.repec.org//p/luc/wpaper/18-13.html},
	abstract = {Fixed-effect models are widely used econometric methods. This paper presents the R package FENmlm, which is devoted to the estimation of maximum likelihood (ML) models with any number of fixed-effects. The core of the algorithm, detailed in the paper, is based on a general framework to estimate any ML model with multiple fixed effects. It also integrates a fixed-point acceleration method to hasten the convergence of the fixed-effect coefficients. The R function offers the user a simple way to estimate any of four different maximum likelihood models: Poisson, Negative Binomial, Gaussian and Logit. Illustrations with real data detail the estimation process as well as the clustering of standard-errors and the various tools to export and manage results from multiple estimations. Simulations show that the algorithm outperforms existing methods in terms of computing time (often by orders of magnitude) or, in the Gaussian case, is on a par with the most efficient ones. Most interestingly, apart from the Gaussian case, the algorithm is revealed to be the only able to estimate models with many fixed-effects on a simple laptop. FENmlm is a free software and distributed under the general public license, as part of the R software project.},
	language = {en},
	urldate = {2025-02-27},
	journal = {DEM Discussion Paper Series},
	author = {Bergé, Laurent},
	year = {2018},
	note = {Number: 18-13
Publisher: Department of Economics at the University of Luxembourg},
	keywords = {fixed-effects, maximum likelihood estimation., R package},
	file = {Snapshot:/home/pacha/Zotero/storage/EJQNYWRS/18-13.html:text/html},
}

@book{escalante,
	series = {Fundamentals of {Algorithms}},
	title = {Alternating {Projection} {Methods}},
	isbn = {978-1-61197-193-4},
	url = {https://epubs.siam.org/doi/book/10.1137/9781611971941},
	urldate = {2025-04-01},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Escalante, René and Raydan, Marcos},
	month = jan,
	year = {2011},
	doi = {10.1137/9781611971941},
	keywords = {acceleration techniques, alternating projection methods, convex feasibility problems, Dykstra's Algorithm, row-action methods},
	file = {PDF:/home/pacha/Zotero/storage/FVKNKF65/Escalante and Raydan - 2011 - Alternating Projection Methods.pdf:application/pdf},
}

@article{irons,
	title = {A version of the {Aitken} accelerator for computer iteration},
	volume = {1},
	copyright = {Copyright © 1969 John Wiley \& Sons, Ltd},
	issn = {1097-0207},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nme.1620010306},
	doi = {10.1002/nme.1620010306},
	language = {en},
	number = {3},
	urldate = {2025-04-01},
	journal = {International Journal for Numerical Methods in Engineering},
	author = {Irons, Bruce M. and Tuck, Robert C.},
	year = {1969},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/nme.1620010306},
	pages = {275--277},
	file = {Full Text PDF:/home/pacha/Zotero/storage/AX8T3RNJ/Irons and Tuck - 1969 - A version of the Aitken accelerator for computer iteration.pdf:application/pdf;Snapshot:/home/pacha/Zotero/storage/G3YYCHE8/nme.html:text/html},
}

@incollection{casella,
	edition = {2},
	title = {Principles of {Data} {Reduction}},
	isbn = {978-1-003-45628-5},
	abstract = {This chapter covers three principles of data reduction – Sufficiency Principle, Likelihood Principle, and Equivariance Principle. The Sufficiency Principle promotes a method of data reduction that does not discard information about the unknown parameter while achieving some summarization of the data. The Likelihood Principle describes a function of the parameter, determined by the observed sample, that contains all the information about the unknown parameter that is available from the sample. The Equivariance Principle prescribes another method of data reduction that still preserves some important features of the model.},
	booktitle = {Statistical {Inference}},
	publisher = {Chapman and Hall/CRC},
	author = {Casella, George and Berger, Roger L.},
	year = {2024},
	note = {Num Pages: 31},
}

@book{aliprantis,
	address = {Berlin},
	title = {Infinite {Dimensional} {Analysis}: {A} {Hitchhiker}'s {Guide}},
	isbn = {978-3-540-29586-0},
	shorttitle = {Infinite {Dimensional} {Analysis}},
	abstract = {This new edition of The Hitchhiker's Guide has bene?tted from the comments of many individuals, which have resulted in the addition of some new material, and the reorganization of some of the rest. The most obvious change is the creation of a separate Chapter 7 on convex analysis. Parts of this chapter appeared in elsewhere in the second edition, but much of it is new to the third edition. In particular, there is an expanded discussion of support points of convex sets, and a new section on subgradients of convex functions. There is much more material on the special properties of convex sets and functions in ?nite dimensional spaces. There are improvements and additions in almost every chapter. There is more new material than might seem at ?rst glance, thanks to a change in font that - duced the page count about ?ve percent. We owe a huge debt to Valentina Galvani, Daniela Puzzello, and Francesco Rusticci, who were participants in a graduate seminar at Purdue University and whose suggestions led to many improvements, especially in chapters ?ve through eight. We particularly thank Daniela Puzzello for catching uncountably many errors throughout the second edition, and simplifying the statements of several theorems and proofs. In another graduate seminar at Caltech, many improvements and corrections were suggested by Joel Grus, PJ Healy, Kevin Roust, Maggie Penn, and Bryan Rogers.},
	language = {English},
	publisher = {Springer/Sci-Tech/Trade},
	author = {Aliprantis, Charalambos D. and Border, Kim C.},
	year = {2006},
}

@article{shoven,
	title = {Applied {General}-{Equilibrium} {Models} of {Taxation} and {International} {Trade}: {An} {Introduction} and {Survey}},
	volume = {22},
	issn = {0022-0515},
	shorttitle = {Applied {General}-{Equilibrium} {Models} of {Taxation} and {International} {Trade}},
	url = {https://www.jstor.org/stable/2725306},
	number = {3},
	urldate = {2024-03-18},
	journal = {Journal of Economic Literature},
	author = {Shoven, John B. and Whalley, John},
	year = {1984},
	note = {Publisher: American Economic Association},
	pages = {1007--1051},
	file = {JSTOR Full Text PDF:/home/pacha/Zotero/storage/PV8RVAFE/Shoven and Whalley - 1984 - Applied General-Equilibrium Models of Taxation and.pdf:application/pdf},
}

@article{santos,
	title = {The {Log} of {Gravity}},
	volume = {88},
	issn = {0034-6535},
	url = {https://doi.org/10.1162/rest.88.4.641},
	doi = {10.1162/rest.88.4.641},
	abstract = {Although economists have long been aware of Jensen's inequality, many econometric applications have neglected an important implication of it: under heteroskedasticity, the parameters of log-linearized models estimated by OLS lead to biased estimates of the true elasticities. We explain why this problem arises and propose an appropriate estimator. Our criticism of conventional practices and the proposed solution extend to a broad range of applications where log-linearized equations are estimated. We develop the argument using one particular illustration, the gravity equation for trade. We find significant differences between estimates obtained with the proposed estimator and those obtained with the traditional method.},
	number = {4},
	urldate = {2024-03-20},
	journal = {The Review of Economics and Statistics},
	author = {Silva, J. M. C. Santos and Tenreyro, Silvana},
	month = nov,
	year = {2006},
	pages = {641--658},
	file = {Full Text PDF:/home/pacha/Zotero/storage/SNLKS86U/Silva and Tenreyro - 2006 - The Log of Gravity.pdf:application/pdf;Snapshot:/home/pacha/Zotero/storage/TCKLW8SY/The-Log-of-Gravity.html:text/html},
}

@misc{sanderson,
	title = {Armadillo: {C}++ library for linear algebra \& scientific computing},
	url = {https://arma.sourceforge.net/speed.html},
	urldate = {2024-10-24},
	author = {Sanderson, Conrad},
	year = {2024},
	file = {Armadillo\: C++ library for linear algebra & scientific computing:/home/pacha/Zotero/storage/ZZM4L7GA/speed.html:text/html},
}

@article{yotov2,
	title = {Gravity for {Undergrads}},
	url = {https://ideas.repec.org//p/drx/wpaper/202519.html},
	abstract = {The gravity equation is the "workhorse" model of international trade and the most popular tool for trade policy analysis. Yet, despite its solid theoretical foundations, remarkable empirical success, intuitive appeal, and ease of implementation, the gravity equation has not received proper introduction and well-deserved coverage that would enable and enhance research by undergraduate students and novices to the gravity literature. The objective of this chapter to introduce the gravity model of trade to the undergraduate student and to translate the theoretical gravity equation into an econometric model that can be used for a wide range of research projects and policy analysis.},
	language = {en},
	urldate = {2025-06-14},
	journal = {Working Papers},
	author = {Yotov, Yoto},
	month = apr,
	year = {2025},
	note = {Number: 202519
Publisher: Center for Global Policy Analysis, LeBow College of Business, Drexel University},
	keywords = {Estimation, Gravity model, Methods, Teaching, Theory},
	file = {Fullext PDF:/home/pacha/Zotero/storage/ZTRH56KE/Yotov - 2025 - Gravity for Undergrads.pdf:application/pdf;Snapshot:/home/pacha/Zotero/storage/7I6T2WHE/202519.html:text/html},
}

@Manual{base,
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  organization = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2025},
  url = {https://www.R-project.org/},
}

@article{wickham,
	title = {testthat: {Get} {Started} with {Testing}},
	volume = {3},
	issn = {2073-4859},
	shorttitle = {testthat},
	url = {https://journal.r-project.org/archive/2011/RJ-2011-002/index.html},
	language = {en},
	number = {1},
	urldate = {2025-06-14},
	journal = {The R Journal},
	author = {Wickham, Hadley},
	year = {2011},
	pages = {5--10},
	file = {Snapshot:/home/pacha/Zotero/storage/SKPGRFCJ/index.html:text/html},
}

@article{correia,
	title = {ppmlhdfe: {Fast} {Poisson} {Estimation} with {High}-{Dimensional} {Fixed} {Effects}},
	volume = {20},
	issn = {1536-867X, 1536-8734},
	shorttitle = {ppmlhdfe},
	url = {http://arxiv.org/abs/1903.01690},
	doi = {10.1177/1536867X20909691},
	abstract = {In this paper we present ppmlhdfe, a new Stata command for estimation of (pseudo) Poisson regression models with multiple high-dimensional fixed effects (HDFE). Estimation is implemented using a modified version of the iteratively reweighted least-squares (IRLS) algorithm that allows for fast estimation in the presence of HDFE. Because the code is built around the reghdfe package, it has similar syntax, supports many of the same functionalities, and benefits from reghdfe's fast convergence properties for computing high-dimensional least squares problems. Performance is further enhanced by some new techniques we introduce for accelerating HDFE-IRLS estimation specifically. ppmlhdfe also implements a novel and more robust approach to check for the existence of (pseudo) maximum likelihood estimates.},
	number = {1},
	urldate = {2025-06-02},
	journal = {The Stata Journal: Promoting communications on statistics and Stata},
	author = {Correia, Sergio and Guimarães, Paulo and Zylkin, Thomas},
	month = mar,
	year = {2020},
	note = {arXiv:1903.01690 [econ]},
	keywords = {Economics - Econometrics},
	pages = {95--115},
	file = {Preprint PDF:/home/pacha/Zotero/storage/AHERGV2T/Correia et al. - 2020 - ppmlhdfe Fast Poisson Estimation with High-Dimensional Fixed Effects.pdf:application/pdf;Snapshot:/home/pacha/Zotero/storage/MNR9EK9Z/1903.html:text/html},
}

@article{kindermann,
	title = {Convergence rates for {Kaczmarz}-type regularization methods},
	volume = {8},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1930-8337},
	url = {https://www.aimsciences.org/en/article/doi/10.3934/ipi.2014.8.149},
	doi = {10.3934/ipi.2014.8.149},
	abstract = {This article is devoted to the convergence analysis of a special family of iterativeregularization methods for solving systems of ill--posed operator equations in Hilbertspaces, namely Kaczmarz-type methods.The analysis is focused on the Landweber--Kaczmarz (LK) explicit iteration and theiterated Tikhonov--Kaczmarz (iTK) implicit iteration. The corresponding symmetricversions of these iterative methods are also investigated (sLK and siTK).We prove convergence rates for the four methods above, extending and complementing theconvergence analysis established originally in [22,13,12,8].},
	language = {en},
	number = {1},
	urldate = {2025-07-05},
	journal = {Inverse Problems and Imaging},
	author = {Kindermann, Stefan and Leitão, Antonio},
	month = mar,
	year = {2014},
	note = {Publisher: Inverse Problems and Imaging},
	pages = {149--172},
}

@article{kendallknight,
	title = {Kendallknight: {An} {R} package for efficient implementation of {Kendall}’s correlation coefficient computation},
	volume = {20},
	issn = {1932-6203},
	shorttitle = {Kendallknight},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0326090},
	doi = {10.1371/journal.pone.0326090},
	abstract = {The kendallknight package introduces an efficient implementation of Kendall’s correlation coefficient computation, significantly improving the processing time for large datasets without sacrificing accuracy. The kendallknight package, following Knight (1966) and posterior literature, reduces the time complexity resulting in drastic reductions in computation time, transforming operations that would take minutes or hours into milliseconds or minutes, while maintaining precision and correctly handling edge cases and errors. The package is particularly advantageous in econometric and statistical contexts where rapid and accurate calculation of Kendall’s correlation coefficient is desirable. Benchmarks demonstrate substantial performance gains over the Base R implementation, especially for large datasets.},
	language = {en},
	number = {6},
	urldate = {2025-06-19},
	journal = {PLOS ONE},
	author = {Vargas Sepulveda, Mauricio},
	month = jun,
	year = {2025},
	note = {Publisher: Public Library of Science},
	keywords = {Econometrics, Algorithms, Computer and information sciences, Computer hardware, Genome complexity, Statistical distributions, Supercomputers, Test statistics},
	pages = {e0326090},
	file = {Full Text PDF:/home/pacha/Zotero/storage/PNTJVG3R/Sepulveda - 2025 - Kendallknight An R package for efficient implementation of Kendall’s correlation coefficient comput.pdf:application/pdf},
}
